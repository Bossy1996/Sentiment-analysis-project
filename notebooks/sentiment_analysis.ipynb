{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[,\n",
       " Dave,\n",
       " watched,\n",
       " as,\n",
       " the,\n",
       " forest,\n",
       " burned,\n",
       " up,\n",
       " on,\n",
       " the,\n",
       " hill,\n",
       " ,,\n",
       " ,\n",
       " only,\n",
       " a,\n",
       " few,\n",
       " miles,\n",
       " from,\n",
       " his,\n",
       " house,\n",
       " .,\n",
       " The,\n",
       " car,\n",
       " had,\n",
       " ,\n",
       " been,\n",
       " hastily,\n",
       " packed,\n",
       " and,\n",
       " Marta,\n",
       " was,\n",
       " inside,\n",
       " trying,\n",
       " to,\n",
       " round,\n",
       " ,\n",
       " up,\n",
       " the,\n",
       " last,\n",
       " of,\n",
       " the,\n",
       " pets,\n",
       " .,\n",
       " \",\n",
       " Where,\n",
       " could,\n",
       " she,\n",
       " be,\n",
       " ?,\n",
       " \",\n",
       " he,\n",
       " wondered,\n",
       " ,\n",
       " as,\n",
       " he,\n",
       " continued,\n",
       " to,\n",
       " wait,\n",
       " for,\n",
       " Marta,\n",
       " to,\n",
       " appear,\n",
       " with,\n",
       " the,\n",
       " pets,\n",
       " .,\n",
       " ]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "text = \"\"\"\n",
    "Dave watched as the forest burned up on the hill,\n",
    "only a few miles from his house. The car had\n",
    "been hastily packed and Marta was inside trying to round\n",
    "up the last of the pets. \"Where could she be?\" he wondered\n",
    "as he continued to wait for Marta to appear with the pets. \n",
    "\"\"\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "token_list = [token for token in doc]\n",
    "token_list\n",
    "# Tokenization of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[,\n",
       " Dave,\n",
       " watched,\n",
       " forest,\n",
       " burned,\n",
       " hill,\n",
       " ,,\n",
       " ,\n",
       " miles,\n",
       " house,\n",
       " .,\n",
       " car,\n",
       " ,\n",
       " hastily,\n",
       " packed,\n",
       " Marta,\n",
       " inside,\n",
       " trying,\n",
       " round,\n",
       " ,\n",
       " pets,\n",
       " .,\n",
       " \",\n",
       " ?,\n",
       " \",\n",
       " wondered,\n",
       " ,\n",
       " continued,\n",
       " wait,\n",
       " Marta,\n",
       " appear,\n",
       " pets,\n",
       " .,\n",
       " ]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removes stop words\n",
    "filtered_tokens = [token for token in doc if not token.is_stop]\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Token: \\n, lemma: 962983613142996970',\n",
       " 'Token: Dave, lemma: 15237984737769454380',\n",
       " 'Token: watched, lemma: 2054481287215635300',\n",
       " 'Token: forest, lemma: 12560106647199032635',\n",
       " 'Token: burned, lemma: 12905682277821018784',\n",
       " 'Token: hill, lemma: 1647358963876657122',\n",
       " 'Token: ,, lemma: 2593208677638477497',\n",
       " 'Token: \\n, lemma: 962983613142996970',\n",
       " 'Token: miles, lemma: 15996833532744392865',\n",
       " 'Token: house, lemma: 9471806766518506264',\n",
       " 'Token: ., lemma: 12646065887601541794',\n",
       " 'Token: car, lemma: 17545852598994811774',\n",
       " 'Token: \\n, lemma: 962983613142996970',\n",
       " 'Token: hastily, lemma: 16524687012062183671',\n",
       " 'Token: packed, lemma: 11929990034961539164',\n",
       " 'Token: Marta, lemma: 3686051643097225522',\n",
       " 'Token: inside, lemma: 3410355712981309345',\n",
       " 'Token: trying, lemma: 4812066089261065646',\n",
       " 'Token: round, lemma: 10404471077220350636',\n",
       " 'Token: \\n, lemma: 962983613142996970',\n",
       " 'Token: pets, lemma: 8199115189604440881',\n",
       " 'Token: ., lemma: 12646065887601541794',\n",
       " 'Token: \", lemma: 15884554869126768810',\n",
       " 'Token: ?, lemma: 8205403955989537350',\n",
       " 'Token: \", lemma: 15884554869126768810',\n",
       " 'Token: wondered, lemma: 17230765341337091640',\n",
       " 'Token: \\n, lemma: 962983613142996970',\n",
       " 'Token: continued, lemma: 7058970474994285413',\n",
       " 'Token: wait, lemma: 6203382976377178934',\n",
       " 'Token: Marta, lemma: 3686051643097225522',\n",
       " 'Token: appear, lemma: 6170484565757444627',\n",
       " 'Token: pets, lemma: 8199115189604440881',\n",
       " 'Token: ., lemma: 12646065887601541794',\n",
       " 'Token: \\n, lemma: 962983613142996970']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalizing words\n",
    "lemmas = [\n",
    "    f\"Token: {token}, lemma: {token.lemma}\"\n",
    "    for token in filtered_tokens\n",
    "]\n",
    "lemmas"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1f427c9b35d746f37a89fb3dfb686a5558ea468b7576db0ae573ddbec2c18f0f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('Sentiment-analysis-project-ObCJVngF': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}